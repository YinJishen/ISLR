---
title: "6 Linear Model Selection and Regulization"
author: "Jishen Yin"
date: "2019/11/29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Lab 1: Subset Selection Methods

## Best Subset Selection

```{r}
library(ISLR)
data(Hitters)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
```

```{r}
Hitters <- na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters))
```

```{r}
library(leaps)
regfit.full <- regsubsets(Salary~., Hitters)
summary(regfit.full)
```

```{r}
regfit.full <- regsubsets(Salary~., data = Hitters, nvmax = 19)
reg.summary <- summary(regfit.full)
```

```{r}
names(reg.summary)
```

```{r}
reg.summary$rsq
```

```{r}
par(mfrow = c(2,2))
plot(reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted Rsq", type = "l")
points(which.max(reg.summary$adjr2), reg.summary$adjr2[which.max(reg.summary$adjr2)], col = "red", cex = 2, pch = 20)
plot(reg.summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
points(which.min(reg.summary$cp), reg.summary$cp[which.min(reg.summary$cp)], col = "red", cex = 2, pch = 20)
plot(reg.summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
points(which.min(reg.summary$bic), reg.summary$bic[which.min(reg.summary$bic)], col = "red", cex = 2, pch = 20)
```

```{r}
plot(regfit.full, scale = "r2")
plot(regfit.full, scale = "adjr2")
plot(regfit.full, scale = "Cp")
plot(regfit.full, scale = "bic")
```

```{r}
coef(regfit.full, 6)
```

## Forward and Backward Stepwise Selection

```{r}
regfit.fwd <- regsubsets(Salary~., data = Hitters, nvmax = 19, method = "forward")
summary(regfit.fwd)
regfit.bwd <- regsubsets(Salary~., data = Hitters, nvmax = 19, method = "backward")
summary(regfit.bwd)
```

```{r}
coef(regfit.full, 7)
coef(regfit.fwd, 7)
coef(regfit.bwd, 7)
```

## Choosing Among Models Using the Validation Set Approach and Cross-Validation

```{r}
set.seed(1)
train <- sample(c(TRUE, FALSE), nrow(Hitters), replace = TRUE)
test <- (!train)
```

```{r}
regfit.best <- regsubsets(Salary~., data = Hitters[train,], nvmax = 19)
```

```{r}
test.mat <- model.matrix(Salary~., data = Hitters[test,])
```

```{r}
val.errors <- rep(NA, 19)
for (i in 1:19) {
  coefi <- coef(regfit.best, id = i)
  pred <- test.mat[, names(coefi)]%*%coefi
  val.errors[i] <- mean((Hitters$Salary[test]-pred)^2)
}
```

```{r}
val.errors
which.min(val.errors)
coef(regfit.best, 10)
```

```{r}
predict.regsubsets <- function(object, newdata, id, ...){
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form, newdata)
  coefi <- coef(object, id = id)
  xvars <- names(coefi)
  mat[, xvars]%*%coefi
}
```

```{r}
regfit.best <- regsubsets(Salary~., data = Hitters, nvmax = 19)
coef(regfit.best, 10)
```

```{r}
k <- 10
set.seed(1)
folds <- sample(1:k, nrow(Hitters), replace = TRUE)
cv.errors <- matrix(NA, k, 19, dimnames = list(NULL, paste(1:19)))
```

```{r}
for (j in 1:k) {
  best.fit <- regsubsets(Salary~., data = Hitters[folds!=j,], nvmax = 19)
  for(i in 1:19){
    pred <- predict(best.fit, Hitters[folds==j,], id = i)
    cv.errors[j, i] <- mean((Hitters$Salary[folds==j]-pred)^2)
  }
}
```

```{r}
mean.cv.errors <- apply(cv.errors, 2, mean)
mean.cv.errors
par(mfrow = c(1,1))
plot(mean.cv.errors, type = "b")
```

```{r}
reg.best <- regsubsets(Salary~., data = Hitters, nvmax = 19)
coef(reg.best, 11)
```

# Lab 2: Ridge Regression and the Lasso

```{r}
x <- model.matrix(Salary~., Hitters)[,-1]
y <- Hitters$Salary
```

## Ridge Regression

```{r, warning=FALSE}
library(glmnet)
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x, y, alpha = 0, lambda = grid)
```

```{r}
dim(coef(ridge.mod))
```

```{r}
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))
```

```{r}
ridge.mod$lambda[60]
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))
```

```{r}
predict(ridge.mod, s = 50, type = "coefficients")[1:20,]
```

```{r}
set.seed(1)
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
y.test <- y[test]
```

```{r}
ridge.mod <- glmnet(x[train,], y[train], alpha = 0, lambda = grid,
                    thresh = 1e-12)
ridge.pred <- predict(ridge.mod, s = 4, newx = x[test,])
mean((ridge.pred-y.test)^2)
```

```{r}
mean((mean(y[train])-y.test)^2)
```

```{r}
ridge.pred <- predict(ridge.mod, s = 1e10, newx = x[test,])
mean((ridge.pred-y.test)^2)
```

```{r}
ridge.pred <- predict(ridge.mod, s = 0, newx = x[test,])
mean((ridge.pred-y.test)^2)
lm(y~x, subset = train)
predict(ridge.mod, s = 0, type = "coefficients")[1:20,]
```

```{r}
set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
```

```{r}
ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[test,])
mean((ridge.pred-y.test)^2)
```

```{r}
out <- glmnet(x, y, alpha = 0)
predict(out, type = "coefficients", s = bestlam)[1:20,]
```

## Lasso

```{r}
lasso.mod <- glmnet(x[train,], y[train], alpha = 1, lambda = grid)
plot(lasso.mod)
```

```{r}
set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha = 1)
plot(cv.out)
bestlam <- cv.out$lambda.min
lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test,])
mean((lasso.pred-y.test)^2)
```

```{r}
out <- glmnet(x, y, alpha = 1, lambda = grid)
lasso.coef <- predict(out, type = "coefficients", s = bestlam)[1:20,]
lasso.coef
lasso.coef[lasso.coef!=0]
```