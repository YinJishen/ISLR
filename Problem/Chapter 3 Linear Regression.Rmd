---
title: "Chapter 3 Linear Regression"
author: "Jishen Yin"
date: "2020/5/3"
output: pdf_document
---

```{r setup, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(MASS)
library(tidyverse)
library(GGally)
```

## Problem 8

This question involves the use of simple linear regression on the `Auto` data set.

(a) Perform a simple linear regression with `mpg` as the response and `horsepower` as the predictor. Comment on the summary.

i. Is there a relationship between the predictor and the response?

ii. How strong is the relationship between the predictor and the response?

iii. Is the relationship between the predictor and the response positive and negative?

```{r}
data(Auto)
model <- lm(mpg ~ horsepower, data = Auto)
summary(model)
```

They have linear relationship. $R^2$ is 0.6059, which indicates that their relationship is not strong. The coefficient is negative, indicating a negative relationship between `mpg` and `horsepower`.

iv. What is the predicted `mpg` associated with a `horsepower` of 98? What are the associated 95% confidence and prediction intervals?

```{r}
predict.lm(model, data.frame(horsepower=98), interval = "prediction", level = 0.95)
```

(b) Plot the response and the predictor. Display the least squares regression line.

```{r}
ggplot(data = Auto) +
  geom_point(aes(x = horsepower, y = mpg)) +
  geom_smooth(aes(x = horsepower, y = mpg), method = "lm", se = FALSE)
```

(c) Produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.

```{r}
par(mfrow=c(2, 2))
plot(model)
```

The variance is not constant among the predictor. Number 323, 330, 334 are potential outliers. Number 94, 117 are potential influential points.

## Problem 9

This question involves the use of multiple linear regression on the `Auto` data set.

(a) Produce a scatterplot matrix which includes all of the variables in the data set.

```{r, message=FALSE}
ggpairs(Auto[, 1:8], axisLabels = "none")
```

(b) Compute the matrix of correlations between the variables.

```{r}
cor(Auto[, 1:8])
```

(c) Perform a multiple linear regression with `mpg` as the response and all other variables except `name`. Comment on the summary.

i. Is there a relationship between the predictors and the response?

ii. Which predictors appear to have a statistically significant relationship to the response?

iii. What does the coefficient for the `year` variable suggest?

```{r}
model <- lm(mpg ~ .-name, data = mutate(Auto, origin = as.factor(origin)))
summary(model)
```

`displacement`, `weight`, `year` and `origin` have significant relationship to the response. On average, `mpg` will increase 0.7 after one year.

(d) Produce diagnostic plots of the linear regression fit. Comment on any problems.

```{r}
par(mfrow = c(2, 2))
plot(model)
```

(e) Fit linear regression models with interaction effects.

```{r}
model2 <- lm(mpg ~ (.-name)^2, data = mutate(Auto, origin = as.factor(origin)))
bic <- step(model2, direction = "both", k = log(nrow(Auto)), trace = 0)
summary(bic)
```

## Problem 10

This question should be answered using the `Carseats` data set.

(a) Fit a multiple regression model to predict `Sales` using `Price`, `Urban`, and `US`.

```{r}
data(Carseats)

reg <- lm(Sales ~ Price + Urban + US, data = Carseats)
summary(reg)
```

(b) Provide an interpretation of each coefficient in the model. Be careful - some of the variables in the model are qualitative.

(c) Write out the model in equation form, being careful to handle the qualitative variables properly.

(d) For which of the predictors can you reject the null hypothesis $H_0:\beta_j=0$?

(e) On the basis of your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of association with the outcome.

```{r}
small <- lm(Sales ~ Price + US, data = Carseats)
summary(small)
```

(f) How well do the models in (a) and (e) fit the data?

(g) Using the model from (e), obtain 95% confidence intervals for the coefficients.

```{r}
confint(small)
```

(h) Is there evidence of outliers or high leverage observations in the model from (e)?

```{r}
par(mfrow = c(2, 2))
plot(small)
```

## Problem 15

This problem involves the `Boston` data set, which we saw in the lab for this chapter. We will now try to predict per capita crime rate using the other variables in this data set. In other words, per capita crime rate is the response, and the other variables are the predictors.

(a) For each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions.

```{r}
data(Boston)
```

```{r}
l <- lapply(colnames(Boston)[2:14], function(x){
  formul <- formula(paste("crim ~", x))
  model <- lm(formul, data = Boston)
  return(data.frame(varname = x,
                    sim_coef = model$coefficients[2],
                    sim_r2 = summary(model)$r.squared))
})

df <- do.call(rbind, l)
```


(b) Fit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis $H_0:\beta_j=0$?

```{r}
full <- lm(crim ~ ., data = Boston)
df["mul_coef"] <- full$coefficients[2:14]
summary(full)
```

(c) How do your results from (a) compare to your results from (b)? Create a plot displaying the univariate regression coefficients from (a) on the $x$-axis, and the multiple regression coefficients from (b) on the $y$-axis. That is, each predictor is displayed as a single point in the plot. Its coefficient in a simple linear regression model is shown on the $x$-axis, and its coefficient estimate in the multiple linear regression model is shown on the $y$-axis.

```{r}
ggplot(data = df) +
  geom_point(aes(x = sim_coef, y = mul_coef)) +
  geom_abline(slope = 1, intercept = 0)
```

(d) Is there evidence of non-linear association between any of the predictors and the response? To answer this question, for each predictor $X$, fit a model of the form

$$
Y=\beta_0+\beta_1X+\beta_2X^2+\beta_3X^3+\epsilon
$$

```{r}
df["mulno_r2"] <- sapply(colnames(Boston)[2:14], function(x){
  formul <- formula(paste("crim ~", x, "+I(", x, "^2)+I(", x, "^3)"))
  model <- lm(formul, data = Boston)
  return(summary(model)$r.squared)
})
```